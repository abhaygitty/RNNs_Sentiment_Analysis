{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file download utilities\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.1\n",
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOADED_FILENAME = 'SampleText.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url_path, expected_bytes):\n",
    "    if not os.path.exists(DOWNLOADED_FILENAME):\n",
    "        filename, _ = urllib.request.urlretrieve(url_path, DOWNLOADED_FILENAME)\n",
    "    \n",
    "    statinfo = os.stat(DOWNLOADED_FILENAME)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified file from this path: ', url_path)\n",
    "        print('Downloaded file: ', DOWNLOADED_FILENAME)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception('Failed to verify file from: ' + url_path + '. Can you get to it from a browser?')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words():\n",
    "    with zipfile.ZipFile(DOWNLOADED_FILENAME) as f:\n",
    "        firstfile = f.namelist()[0]\n",
    "        filestring = tf.compat.as_str(f.read(firstfile))\n",
    "        words = filestring.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31344016\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed to verify file from: http://mattmahoney.net/dc/text8.zip. Can you get to it from a browser?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-17a4a1a18103>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mFILESIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m313344016\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmaybe_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFILESIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-5832920830f1>\u001b[0m in \u001b[0;36mmaybe_download\u001b[1;34m(url_path, expected_bytes)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Failed to verify file from: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murl_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Can you get to it from a browser?'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Failed to verify file from: http://mattmahoney.net/dc/text8.zip. Can you get to it from a browser?"
     ]
    }
   ],
   "source": [
    "URL_PATH = 'http://mattmahoney.net/dc/text8.zip'\n",
    "FILESIZE = 313344016\n",
    "\n",
    "maybe_download(URL_PATH, FILESIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = read_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    word_counts = [['UNKNOWN', -1]]\n",
    "    \n",
    "    counter = collections.Counter(words)\n",
    "    word_counts.extend(counter.most_common(n_words-1))\n",
    "    \n",
    "    dictionary = dict()\n",
    "    \n",
    "    for word, _ in word_counts:\n",
    "        dictionary[word] = len(dictionary)#assign word to the dictionary length at this step\n",
    "        #the most common word gets the least index. as word_counts contains words from highest to lowest occurence\n",
    "        \n",
    "    word_indexes = list()\n",
    "    \n",
    "    unknown_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0 #dictionary['UNKNOWN]\n",
    "            unknown_count += 1\n",
    "        \n",
    "        word_indexes.append(index)#all the words in their index form with Zero for the words except the top n_words\n",
    "        \n",
    "    word_counts[0][1] = unknown_count\n",
    "    \n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    #maps values to its coutns {1:\"is\", 2356: \"how\", 43:\"this\"}\n",
    "    \n",
    "    return word_counts, word_indexes, dictionary, reversed_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 5000\n",
    "\n",
    "word_counts, word_indexes, dictionary, reversed_dictionary = build_dataset(vocabulary, VOCABULARY_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['UNKNOWN', 2735459],\n",
       " ('the', 1061396),\n",
       " ('of', 593677),\n",
       " ('and', 416629),\n",
       " ('one', 411764),\n",
       " ('in', 372201),\n",
       " ('a', 325873),\n",
       " ('to', 316376),\n",
       " ('zero', 264975),\n",
       " ('nine', 250430)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_indexes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sight : 4426\n",
      "sons : 2112\n",
      "money : 815\n",
      "forming : 2834\n",
      "yellow : 2458\n",
      "was : 18\n",
      "battery : 4239\n",
      "top : 563\n",
      "cook : 3566\n",
      "binding : 4060\n"
     ]
    }
   ],
   "source": [
    "for key in random.sample(list(dictionary), 10):\n",
    "    print(key, \":\", dictionary[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3403 : campbell\n",
      "4462 : retrieved\n",
      "4265 : hosts\n",
      "3710 : superman\n",
      "2118 : bwv\n",
      "1973 : count\n",
      "4620 : capabilities\n",
      "848 : origin\n",
      "1420 : christians\n",
      "4838 : enabled\n"
     ]
    }
   ],
   "source": [
    "for key in random.sample(list(reversed_dictionary), 10):\n",
    "    print(key, \":\", reversed_dictionary[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global index into words maintained acroos batches\n",
    "global_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(word_indexes, batch_size, num_skips, skip_window):\n",
    "    #num_skips : the number of words we choose from a context window\n",
    "    #num_skips number of words are chosen at random as target word which will be predicted using the context window center word.\n",
    "    #skip_window : number of neighbors we want to consider to the left and to the right\n",
    "    # if skip_window = 3 then context window contains 3 words to the left and 3 words to the right\n",
    "    # The quick brown fox jumped over the lazy dog\n",
    "    # if fox is center word oof the context window\n",
    "    # then we pick num_skips i.e., 2 words at random to be predicted as a target word using center word \"fox\"\n",
    "    # therefore neural network we'd set up uses  \"fox\" to predict -> \"jumped\" and \"fox\" -> \"quick\" jumped and quick are chosen at random\n",
    "\n",
    "    global global_index # where we are in the document\n",
    "    \n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size,1), dtype=np.int32)\n",
    "    #labels : array of arrays containing the indexes of the target predicted words\n",
    "    #from the above example it would be the count of the target words, jumped and quick\n",
    "    #[[1], [0], [0], ..... <batch_size>]\n",
    "    \n",
    "    span = 2 * skip_window + 1 # [skip_window input_word skip_window]\n",
    "    #if the total size of the context window skip_window words on the left skip_wondw words on the right and the center word.\n",
    "    buffer = collections.deque(maxlen = span)# double ended queue\n",
    "    #very fast addition and removal of words from either ends\n",
    "    \n",
    "    for _ in range(span):\n",
    "        buffer.append(word_indexes[global_index])\n",
    "        global_index = (global_index + 1) % len(word_indexes)#making sure to reset global_index back to zero once done with span items\n",
    "    \n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window #input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        \n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span-1)\n",
    "            \n",
    "            targets_to_avoid.append(target)\n",
    "            \n",
    "            batch[i * num_skips + j] = buffer[skip_window] #this is the input word\n",
    "            labels[i * num_skips + j, 0] = buffer[target] # these are the context words\n",
    "        \n",
    "        buffer.append(word_indexes[global_index])\n",
    "        global_index = (global_index + 1) % len(word_indexes)\n",
    "        \n",
    "    global_index = (global_index + len(word_indexes) - span) % len(word_indexes)\n",
    "    \n",
    "            \n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, labels = generate_batch(word_indexes, 10, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,    2, 3134, 3134,   46,   46,   59,   59,  156,  156])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 195],\n",
       "       [  46],\n",
       "       [   6],\n",
       "       [ 195],\n",
       "       [ 742],\n",
       "       [ 156],\n",
       "       [   0],\n",
       "       [ 195],\n",
       "       [3134],\n",
       "       [ 742]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of : term\n",
      "of : first\n",
      "abuse : a\n",
      "abuse : term\n",
      "first : working\n",
      "first : against\n",
      "used : UNKNOWN\n",
      "used : term\n",
      "against : abuse\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    print(reversed_dictionary[batch[i]], \":\", reversed_dictionary[labels[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the global index backk to zero\n",
    "global_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 16\n",
    "valid_window = 100\n",
    "\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #128 input words fed in and their targets correspondingly\n",
    "embedding_size = 50 #number of dimensions our embedding will have. that's 50 neurons in the hidden layer of the neural network\n",
    "skip_window = 2\n",
    "num_skips = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()#fresh tensorflow graph\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])#every iteration = 128 bits of data fed in\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([VOCABULARY_SIZE, embedding_size], -1.0, 1.0))\n",
    "#embeddings are generated using training data set and stored in variables \n",
    "#the shape = vocabsize:50 => every word in vocab has an embedding\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_4:0' shape=(5000, 50) dtype=float32_ref>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup_3:0' shape=(128, 50) dtype=float32>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the hidden layer will be setup usng a linear function with a linear equation \n",
    "# y = wx + b \n",
    "#where w is the set of weights or the weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.truncated_normal([VOCABULARY_SIZE, embedding_size], stddev=1.0/math.sqrt(embedding_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biases are also initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = tf.Variable(tf.zeros([VOCABULARY_SIZE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden layer is generated by hidden = (embed_matrix * weights_matrix) + biases\n",
    "# transpose is to make them compatible with matrix multiplication as both the matrices have the same number of columns whic is the embedding size\n",
    "#transposing maakes matrix1's columns match matrix2's row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_out = tf.matmul(embed, tf.transpose(weights)) + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=(128, 5000) dtype=float32>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order of embed 128x50\n",
    "#order of weights 5000x50 order of transpose of weights = 50x5000\n",
    "#order of hidden_out 128x5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_one_hot = tf.one_hot(train_labels, VOCABULARY_SIZE)\n",
    "\n",
    "#the input is converted into a one hot notation in order to be able to use the \n",
    "# SOFTMAX prediction layer at the output to interpret the hidden layer's output and give us the actual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, \n",
    "                                                              labels=train_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax prediction layer actually is slow and uses the cross entropy as the loss fucntion in order to minimise the error in the output\n",
    "#uses the logit function 1/(1+e^(wx+B)) for one binary output and \n",
    "# uses 1/(e^-(wx+B)) fot the other binary output\n",
    "#it can be further extended to any number of output probablities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#further this softmax's output is fed into a loss minimsation function that uses internally the gradient descent funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross entropy as a loss function is the measure of the matching between the actual and the predicted probability distributions\n",
    "#More the difference => more the entropy => more the uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to be reduced. hence fed into the gradient descent optimizer with a step size of 0.1 to minimize the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity is useed\n",
    "#l2 norms are to be calculated first for that \n",
    "#cosine(a,b) = a.b/(||a||.||b||)\n",
    "#||a|| is the l2 norm of a which is nothing but the squre root of the summ of the squares of the vector\n",
    "# if a = [1 2 3] l2norm(a) = sqrt(1^2 + 2^2 + 3^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "\n",
    "normalized_embeddings = embeddings / l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup_4:0' shape=(16, 50) dtype=float32>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'truediv:0' shape=(5000, 50) dtype=float32>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gives the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  0 : 8.616981506347656\n",
      "Nearest to which:  tropical, briefly, historic, algorithm, steel, achieved, province, respective,\n",
      "Nearest to most:  sex, types, benjamin, avoid, colour, bringing, waters, whether,\n",
      "Nearest to after:  taiwan, metro, punishment, signature, mathematician, might, cape, integers,\n",
      "Nearest to and:  daily, contribution, inuit, twice, me, strict, invented, models,\n",
      "Nearest to s:  charles, phenomenon, complete, john, sky, genesis, afc, sharp,\n",
      "Nearest to system:  slavic, bibliography, equipped, shot, hamilton, vessels, piano, ford,\n",
      "Nearest to all:  showing, elected, cd, si, substances, describe, drink, right,\n",
      "Nearest to a:  general, actual, issued, board, types, norway, kernel, explained,\n",
      "Nearest to more:  designed, arranged, storm, jacob, method, operator, did, differences,\n",
      "Nearest to about:  caused, multi, compared, function, existence, ghost, begin, statistical,\n",
      "Nearest to other:  farmers, annual, industries, max, apollo, doctrine, christmas, territories,\n",
      "Nearest to four:  driving, roles, material, capacity, composition, lies, stroke, smith,\n",
      "Nearest to between:  successful, long, eye, released, coding, succession, primitive, below,\n",
      "Nearest to their:  peru, ip, composed, germanic, manufacturing, dynasty, nations, ranges,\n",
      "Nearest to were:  weapons, establishing, twentieth, pronounced, shortly, career, microwave, eastern,\n",
      "Nearest to its:  child, degrees, issue, prior, theatre, jr, graduate, influential,\n",
      "\n",
      "\n",
      "Average loss at step  200 : 8.107295384407044\n",
      "Average loss at step  400 : 7.453453299999237\n",
      "Average loss at step  600 : 7.294765300750733\n",
      "Average loss at step  800 : 6.961680312156677\n",
      "Average loss at step  1000 : 6.4885994553565975\n",
      "Nearest to which:  tropical, briefly, historic, algorithm, province, achieved, steel, dna,\n",
      "Nearest to most:  sex, types, benjamin, avoid, colour, bringing, waters, whether,\n",
      "Nearest to after:  taiwan, metro, punishment, signature, mathematician, might, cape, integers,\n",
      "Nearest to and:  daily, contribution, inuit, twice, me, invented, integers, encoding,\n",
      "Nearest to s:  charles, phenomenon, complete, john, sky, afc, sharp, genesis,\n",
      "Nearest to system:  slavic, equipped, bibliography, shot, hamilton, vessels, piano, ford,\n",
      "Nearest to all:  showing, elected, si, cd, substances, describe, drink, right,\n",
      "Nearest to a:  general, actual, issued, board, types, norway, kernel, explained,\n",
      "Nearest to more:  designed, arranged, storm, jacob, method, operator, did, differences,\n",
      "Nearest to about:  caused, compared, multi, function, existence, ghost, begin, used,\n",
      "Nearest to other:  farmers, annual, industries, max, apollo, doctrine, christmas, territories,\n",
      "Nearest to four:  driving, roles, material, capacity, composition, lies, meet, smith,\n",
      "Nearest to between:  successful, long, eye, released, coding, succession, primitive, below,\n",
      "Nearest to their:  peru, ip, composed, germanic, manufacturing, dynasty, nations, van,\n",
      "Nearest to were:  weapons, establishing, twentieth, pronounced, shortly, career, microwave, eastern,\n",
      "Nearest to its:  child, degrees, issue, prior, theatre, jr, graduate, influential,\n",
      "\n",
      "\n",
      "Average loss at step  1200 : 6.81009430885315\n",
      "Average loss at step  1400 : 6.562473628520966\n",
      "Average loss at step  1600 : 6.632505885362625\n",
      "Average loss at step  1800 : 6.527580182552338\n",
      "Average loss at step  2000 : 6.396767802238465\n",
      "Nearest to which:  tropical, briefly, historic, algorithm, province, achieved, steel, dna,\n",
      "Nearest to most:  sex, types, benjamin, avoid, colour, bringing, whether, waters,\n",
      "Nearest to after:  taiwan, metro, punishment, signature, mathematician, might, cape, integers,\n",
      "Nearest to and:  daily, contribution, inuit, invented, me, twice, integers, encoding,\n",
      "Nearest to s:  charles, phenomenon, complete, john, afc, sky, sharp, genesis,\n",
      "Nearest to system:  slavic, equipped, bibliography, shot, hamilton, vessels, piano, ford,\n",
      "Nearest to all:  showing, elected, si, cd, substances, describe, drink, much,\n",
      "Nearest to a:  general, actual, issued, board, norway, types, kernel, explained,\n",
      "Nearest to more:  designed, arranged, storm, jacob, method, operator, did, differences,\n",
      "Nearest to about:  caused, compared, function, multi, existence, ghost, begin, used,\n",
      "Nearest to other:  farmers, annual, industries, max, apollo, doctrine, buried, territories,\n",
      "Nearest to four:  driving, material, roles, objects, lies, meet, composition, smith,\n",
      "Nearest to between:  successful, long, eye, released, coding, succession, primitive, failed,\n",
      "Nearest to their:  peru, ip, composed, germanic, manufacturing, dynasty, nations, van,\n",
      "Nearest to were:  weapons, establishing, twentieth, pronounced, shortly, career, microwave, eastern,\n",
      "Nearest to its:  child, degrees, issue, prior, theatre, jr, graduate, influential,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(word_indexes, batch_size, num_skips, skip_window)     \n",
    "        \n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "        \n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step%200 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 200\n",
    "                \n",
    "            print('Average loss at step ', step, ':', average_loss)\n",
    "            average_loss = 0\n",
    "            \n",
    "        #Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 1000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            \n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reversed_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of  nearest neighbors\n",
    "                \n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log_str = 'Nearest to %s: ' % valid_word\n",
    "                \n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reversed_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "            print(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
